cerebras:
  save_iter_state_path: /cb/tests/cluster-mgmt/modelzoo-deps/charles/modelzoo_g42_shared/iter_state/230907_mbzuai_hector_codellm_pol1_12_32_phase2_fim
eval_input:
  batch_size: 32
  data_dir: /cb/customers/g42/datasets/llama/slimpj_arxiv_val_packed
  data_processor: GptHDF5MapDataProcessor
  num_workers: 1
  repeat: false
  shuffle: false
  use_worker_cache: false
  vocab_size: 32032
model:
  attention_dropout_rate: 0.0
  attention_kernel: optimized_beta
  attention_softmax_fp32: true
  attention_type: scaled_dot_product
  boundary_casting: false
  dropout_rate: 0.0
  embedding_initializer:
    a: -0.146
    b: 0.146
    mean: 0.0
    name: truncated_normal
    std: 0.073
  embeddings_scale: 14.6
  filter_size: 10922
  hidden_size: 4096
  initializer:
    a: -0.0365
    b: 0.0365
    mean: 0.0
    name: truncated_normal
    std: 0.01825
  layer_norm_epsilon: 1.0e-05
  loss_scaling: batch_size
  loss_weight: 0.00048828125
  max_position_embeddings: 2048
  mixed_precision: true
  nonlinearity: swiglu
  num_heads: 32
  num_hidden_layers: 32
  output_layer_initializer:
    a: -0.0045625
    b: 0.0045625
    mean: 0.0
    name: truncated_normal
    std: 0.00228125
  output_logits_scale: 0.13875
  position_embedding_type: rotary
  rotary_dim: 32
  scale_glu_initialization: true
  scale_qk_dot_by_d: true
  share_embedding_weights: true
  use_bfloat16: true
  use_bias_in_output: false
  use_ffn_bias: true
  use_ffn_bias_in_attention: true
  use_position_embedding: true
  use_projection_bias_in_attention: true
  vocab_size: 32032
  weight_initialization_seed: 1
optimizer:
  adjust_learning_rate:
    decoder_kernel: 0.0625
  beta1: 0.9
  beta2: 0.95
  correct_bias: true
  eps: 1.0e-09
  learning_rate:
  - base_lr: 0.0
    decay_steps: 86
    end_learning_rate: 0.0087825
    initial_learning_rate: 0.0
    learning_rate: 0.0
    scheduler: Linear
    steps: 86
  - base_lr: 0.0087825
    decay_steps: 214301
    end_learning_rate: 0.00013679
    initial_learning_rate: 0.0087825
    learning_rate: 0.0087825
    scheduler: Linear
    steps: 214301
  log_summaries: true
  loss_scaling_factor: 1.0
  max_gradient_norm: 1.0
  optimizer_type: AdamW
  weight_decay_rate: 0.1
runconfig:
  checkpoint_path: null
  checkpoint_steps: 1500
  compile_dir: null
  compile_only: null
  credentials_path: /opt/cerebras/certs/tls.crt
  debug_args_path: debug_args_prec_table.proto
  disable_version_check: null
  dist_addr: localhost:8888
  dist_backend: nccl
  enable_distributed: false
  eval_steps: 382
  experimental_api: false
  init_method: env://
  is_pretrained_checkpoint: false
  job_labels:
  - Model=6p7B
  - Name=Hector
  - Organization=MBZUAI
  - Mode=Train
  - Num_CSX=16
  - Language=English
  - Type=Pretraining
  - Dataset=SlimPJ
  job_time_sec: null
  log_steps: 1
  logging: null
  max_steps: 214387
  mgmt_address: null
  mgmt_namespace: rel-191-g42
  mode: train
  model_dir: model_dir_phase2_prec_table
  mount_dirs:
  - /cb/customers
  - /cb/tests
  - /g42_shared
  num_act_servers: 1
  num_csx: 16
  num_epochs: null
  num_steps: null
  num_wgt_servers: null
  num_workers_per_csx: 1
  precision_opt_level: -1
  python_paths:
  - /cb/tests/cluster-mgmt/modelzoo-deps/charles/modelzoo_g42_shared
  save_initial_checkpoint: false
  save_losses: true
  seed: 1
  service_dir: model_dir_phase2_prec_table
  target_device: CSX
  use_appliance_data: true
  use_cs_grad_accum: true
  validate_only: null
train_input:
  batch_size: 2112
  data_processor: GptHDF5MapDataProcessor
  mixture:
  - data_dir: /cb/customers/g42/datasets/llama/SlimPajama/ArXiv_train_packed_part1of2
    weight: 0.016083857723861224
  - data_dir: /cb/customers/g42/datasets/llama/SlimPajama/Book_train_packed_part1of2
    weight: 0.015358576352403003
  - data_dir: /cb/customers/g42/datasets/llama/SlimPajama/C4_train_packed_part1of2
    weight: 0.09909981505117808
  - data_dir: /cb/customers/g42/datasets/llama/SlimPajama/CommonCrawl_train_packed_part1of2
    weight: 0.1983897689052003
  - data_dir: /cb/customers/g42/datasets/llama/SlimPajama/Github_train_packed_part1of2
    weight: 0.018454606218343703
  - data_dir: /cb/customers/g42/datasets/llama/SlimPajama/StackExchange_train_packed_part1of2
    weight: 0.011784941022274698
  - data_dir: /cb/customers/g42/datasets/llama/SlimPajama/Wikipedia_train_packed_part1of2
    weight: 0.012710859995188247
  - data_dir: /g42_shared/datasets/llama/StarCoder_fim_shuffled
    weight: 0.6281175747315507
  num_workers: 1
  persistent_workers: true
  prefetch_factor: 10
  repeat: true
  shuffle: false
  shuffle_seed: 1
  use_worker_cache: false
  vocab_size: 32032
